### 1.比较Boosting与Bagging的异同
二者都是**集成学习算法**，都是将多个弱学习器组合成强学习器的方法。
**Bagging**：从原始数据集中每一轮都有放回地抽取训练集，训练得到k个弱学习器，将这k个弱学习器以**投票**的方式得到最终的分类结果。
**Boosting**：每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重，训练得到k个弱分类器，他们都有各自的权重，通过加权组合的方式得到最终的分类结果。

### 2.无监督学习中存在过拟合吗？
**存在**。我们可以使用无监督学习的某些指标或人为地去评估模型性能，以此来判断是否过拟合。

### 3.什么是k折交叉验证？
将原始数据集划分为k个子集，将其中一个子集作为验证集，其余k-1个子集作为训练集，如此训练和验证一轮称为以此交叉验证。交叉验证重复k次，每个子集都做一次验证集，得到k个模型，加权平均k个模型的结果作为评估整体模型的依据。

### 4.关于k折交叉验证，需要注意什么？
k越大，不一定效果越好，而且越大的k会加大训练时间；在选择k时，需要考虑最小化数据集之间的方差，比如对于2分类任务，采用2分类任务，采用2折交叉验证，即将原始数据集对半分，若此时训练集中都是A类别，验证集中都是B类别，则交叉验证效果会非常差。

### 5.对于一个二分类问题，我们定义超过阈值t的判定为正例，否则判定为负例。现在若将t增大，则准确率和召回率会如何变化。

准确率=TP/(TP+FP)，召回率=TP/(TP+FN);
**TP**：将正例正确分类为正例的数量；
**FP**：表示将负例错误分类为正例的数量；
**FN**：表示将正例错误分类为负例的数量；

准确率可以理解为在所有分类为正例的样例中，分类正确的样本所占比例；
召回率可以理解为在所有原始数据中的正例样品中，正确挑出的正例样本的比例。

因此若增大阈值t，更多不确定（分类概率较小）的样本将会被分为负例，剩余确定（分类概率较大）的样本所占的比例将会增大（或不变），即正确率会增大（或不变）；若增大阈值t，则可能将部分不确定（分类概率较小）的正例样品误分类为负例，即召回率会减小（或不变）。

### 6.以下关于神经网络的说法中，正确的是（C）？
**A**.增加网络层数，总能减小训练集错误率；
**B**.减小网络层数，总能减小测试集错误率；
**C**.增加网络层数，可能增加测试集错误率；

### 7.说明Lp范数间的区别
**L1范数**：向量中各个元素绝对值之和；
**L2范数**：向量中各个元素平方和的开二次根方；
**Lp范数**：向量中各个元素绝对值的p次方和的开p次方跟；

### 8.用梯度下降训练神经网络的参数，为什么参数有时会被训练为nan值？
输入数据本身存在nan值，或者梯度爆炸了（可以降低学习率、或者设置梯度的阈值）

### 9.卷积神经网络CNN中池化层有什么作用？
减小图像尺寸，即数据降维，缓解过拟合，保持一定程度的旋转和平移不变性。

卷积神经网络为什么具有平移不变性？
**不变性**意味着即使目标的外观发生了某种变化，但是你依然可以把它识别出来。
各种不变性：
**平移不变性**：Translation Invariance
**旋转/视角不变性**：Ratation/Viewpoint Invariance
**尺度不变性**：Size Invariance
**光照不变性**：Illumination Invariance

平移不变性意味着系统产生完全相同的响应（输出），不管它的输入是如何平移的。

为什么卷积神经网络具有平移不变性？
简单地说，**卷积**+**池化**约等于平移不变性。
**卷积**：简单地说，图像经过平移，响应的特征图上的表达也是平移的。在神经网络中，卷积被定义为不同位置的特征检测器，也就意味着，无论目标出现在图像中的哪个位置，它都会检测到同样的这些特征，输出同样的响应。比如人脸被移动到了图像的左下角，卷积核直到移动到左下角的位置才会检测到它的特征。
**池化**：比如最大池化，它返回感受野中的最大值，如果最大值被移动了，但是仍是在这个感受野中，那么池化层也仍然会输出相同的最大值。这就保证了一定程度的平移不变性。
所以卷积加池化这两种操作共同提供了一些平移不变性，即使图像被平移，卷积保证仍然能检测到它们的特征，池化则尽可能地保证一致的表达。

### 10.请列举出几种常见的激活函数。激活函数有什么作用？
sigmoid, relu, tanh.
激活函数用来使模型非线性化。

### 11.神经网络中Dropout的作用？具体是怎么实现的？
防止过拟合。每次训练，都对每个神经网络单元，按一定概率临时丢弃。

### 12.利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
很有可能产生了梯度消失。它表示神经网络更新时，有些权值不更新，因为梯度接近于0。可以改变激活函数，改变权值的初始化等等。

### 13.如何解决不平衡数据集的分类问题？
可以扩充数据集，对数据重新采样，改变评价指标等。

### 14.残差网络为什么能做到很深层？
神经网络在反向传播过程中要不断地传播梯度，而当网络层数加深时，梯度在逐层传播过程中会导致衰减（求导），导致无法对前面网络层的权重进行有效的调整。残差网络中，加入了short connections为梯度带来了一个直接向前面层的传播通道，缓解了梯度的减小问题。

### 15.相比sigmoid激活函数，ReLU激活函数有什么优势？
（1）防止梯度消失（sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接进于0）
（2）ReLU的输出具有稀疏性（负数都置为0）；
（3）ReLU函数简单，计算速度快；

### 16.卷积神经网络中空洞卷积的作用是什么？
**空洞卷积**也叫**扩张卷积（dilated convolution）**，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为2的3x3卷积核，感受野与5x5的卷积核相同，但参数数量仅为九个。

### 17.解释下卷积神经网络中感受野的概念？
在卷积神经网络中，感受野（receptive field）的定义是：卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上映射的区域大小。

### 18.模型欠拟合什么情况下会出现？有什么解决方案？
模型复杂度过低，不能很好的拟合所有的数据
增加模型复杂度，如采用高阶模型（预测）或者引入更多特征（分类）等

### 19.适用于移动端部署的网络结构有哪些？
Mobilenet（https://arxiv.org/abs/1704.04861）
Shufflenet（https://arxiv.org/abs/1707.01083）
Xception（https://arxiv.org/abs/1610.02357）

### 20.卷积神经网络中im2col是如何实现的？
使用im2col的方法将划窗卷积转为两个大的矩阵相乘；

### 21.多任务学习中标签缺失如何处理？
一般做法是将缺失的标签设置为特殊标志，在计算梯度的时候忽略。

### 22.梯度爆炸的解决方法？
针对梯度爆炸问题，解决方案是引入Gradient Clipping(梯度裁剪)。通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。

### 23.深度学习模型参数初始化都有哪些方法？
（1）Gaussian满足mean=0，std=1的高斯分布x~N（mean, std^2）
（2）Xavier满足x~U（-a，+a）的均匀分布，其中a=sqrt(3/n)
（3）MSRA满足x~N（0，σ^2）的高斯分布，其中σ=sqrt(2/n)
（4）Uniform满足min=0，max=1的均匀分布。x~U（min，max）

### 24.注意力机制在深度学习中的作用是什么？有哪些场景会使用？
深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标是从大量信息中有选择性地筛选出少量最重要信息并聚焦到这些重要信息上，忽略大多不重要的信息。

目前在神经机器翻译（NMT）、图像理解（image caption）等场景都有广泛应用。

### 25.卷积神经网络为什么会具有平移等不变性？
Maxpooling（最大池化）能保证卷积神经网络在一定范围内平移特征能得到同样的激励，具有平移不变性。

### 26.神经网络参数共享（parameter sharing）是指什么？
所谓的权值共享就是说，用一个卷积核去卷积一张图，这张图每个位置是被同样数值的卷积核操作的，权重是一样的，也就是参数共享？

### 27.如何提高小型网络的精度？
（1）进行知识蒸馏（大网络指导小网络）；
（2）利用AutoML进行网络结构的优化；

### 28.什么是神经网络的梯度消失问题，为什么会有梯度消失问题？有什么办法能缓解梯度消失问题？
在反向传播算法计算每一层的误差项时，需要乘以本层激活函数的导数值，如果导数值接近于0，则多次乘积之后误差项会趋向于0，而参数的梯度值通过误差项计算，这会导致参数的剃度值接近于0，无法用梯度下降法来有效的更新参数的值。

改进激活函数，选用更不容易饱和的函数，如ReLU函数。

### 29.列举出你所知道的神经网络中使用的损失函数。
欧氏距离，交叉熵，**对比损失**，**合页损失**

### 30.对于多分类问题，为什么神经网络一般使用交叉熵而不使用欧式距离损失？
**交叉熵**在一般情况下更容易收敛到一个更好的解。

### 31.1x1卷积有什么用途？
**通道降维**，保证卷积神经网络可以接受任何尺寸的输入数据

### 32.随机梯度下降法，在每次迭代时能保证目标函数值一定下降吗？为什么？
不能，每次迭代时目标函数不一样

### 33.梯度下降法，为什么要设置一个学习率？
使得迭代之后的值在上次值的邻域内，保证可以忽略泰勒展开中的二次及二次以上的项

### 34.解释梯度下降法中动量项的作用
利用之前迭代时的梯度值，减小震荡。

### 35.为什么现在倾向于用小尺寸的卷积核？
用多个小卷积核串联可以用大卷积核同样的能力，而且参数更少，另外有更多次的激活函数作用，增强非线性。

### 36.解释GoogleNet的Inception模块的原理
对输入图像用多个不同尺度的卷积核、池化操作进行同时处理，然后将输出结果按照通道拼接起来。

### 37.解释反卷积的原理和用途
**原理**：反卷积即转置卷积，正向传播时乘以卷积核的转置矩阵，反向传播时乘以卷积核矩阵
**用途**：由卷积输出结果近似重构输入数据，上采样

### 38.解释批量归一化（BN）的原理
在数据送入神经网络的某一层进行处理之前，对数据做归一化。按照训练样本的batch进行处理，先减掉这批样本的均值，然后除以标准差，然后进行缩放和平移。缩放和平移参数由训练得到。预测时使用训练时确定的这些值来计算。

### 39.解释SVM核函数的原理
核函数将数据映射到更高维的空间后处理，但不用做这种显示映射，而是先对两个样本向量做内积，然后用核函数映射。这等价于先进行映射，然后再做内积。

### 40.什么是过拟合，过拟合产生的原因是什么？有什么方法能减轻过拟合？
过拟合是指在训练集上表现的很好，但在测试集上表现很差，推广泛化能力差。产生过拟合的原因是训练样本的抽样误差，训练时拟合了这种误差。
方法：增加训练样本，尤其是样本的代表性；正则化。

### 41.什么样的函数可以用作激活函数？
非线性，几乎处处可导，单调

### 42.什么是鞍点问题？
梯度为0，Hessian矩阵不定的点，不是极值点

### 43.在训练深度神经网络的过程中，遇到过那些问题，怎么解决的？
不收敛，收敛太慢，泛化能力差。
调整网络结构，调整样本，调整学习率，调整参数初始化策略。

### 44.SVM如何解决多分类问题
多个二分类器组合。1对1方案，1对剩余方案，多类损失函数。

### 45.列举你知道的聚类算法
层次聚类，k均值算法，DBSCAN算法，OPTICS算法，谱聚类

### 46.K均值算法中，初始类中心怎么确定
随机选择K个样本作为类中心，将样本随机划分为K个子集然后计算类中心。

### 47.简述EM算法的原理
EM算法用于求解带有**隐变量**的最大似然估计问题。由于有隐变量的存在，无法直接用最大似然估计求得对数似然函数极大值的公式解。此时通过jensen不等式构造对数似然函数的下界函数，然后优化下界函数，再用估计出的参数值构造新的下界函数，反复直至收敛到局部极小值点。

### 48.为什么随机森林能降低方差？
随机森林的预测输出值是多课决策树的均值，如果有n个独立同分布的随机变量xi，它们的方差都为σ^2，则它们的均值的方差为：
$$D\left(\frac{1}{n}\sum_{i=1}^nx_i\right)=\frac{\sigma^2}{n}$$

### 49.对于带等式和不等式约束的优化问题，KKT条件是取得极值的充分条件还是必要条件？对于SVM呢？
对于一个一般的问题，KKT条件是取得极值的必要条件而不是充分条件。对于凸优化问题，则是充分条件，**SVM是凸优化问题**。

### 50.解释维数灾难的概念
当特征向量维数很少时，增加特征，可以提高算法的精度，但当特征向量的维数增加到一定数量之后，再增加特征，算法的精度反而会下降。

### 51.Logistic回归为什么用交叉熵而不用欧式距离做损失函数？
如果用欧氏距离，不是凸函数，而用交叉熵则是凸函数

### 52.解释hinge loss损失函数
如果样本没有违反不等式约束，则损失为0；如果违反约束，则有一个正的损失值。

### 53.解释GBDT的核心思想
用加法模拟，更准确的说，是多课决策树来拟合一个目标函数。每一棵决策树拟合的是之前迭代得到的模型的残差。求解的时候，对目标函数使用了一阶泰勒展开，用梯度下降法来训练决策树。

### 54.解释XGBoost的核心思想
在GBDT的基础上，目标函数增加了正则化项，并且在求解时做了二阶泰勒展开

### 55.解释DQN中的经验回放机制，为什么需要这种机制？
将执行动作后得到的状态转移构造的样本存储在一个列表中，然后从中随机抽样，来训练Q网络。为了解决训练样本之间的相关性，以及训练样本分布变化的问题。

### 56.什么是反卷积？
反卷积也称为转置卷积，如果用矩阵乘法实现卷积操作，将卷积核平铺为矩阵，则转置卷积在正向计算时左乘这个矩阵的转置W^T，在反向传播时左乘W，与卷积操作刚好相反，需要注意的是，**反卷积不是卷积的逆运算**。

### 57.反卷积有哪些用途？
实现上采样；近似重构输入图像，卷积层可视化。

### 58.PCA（主成分分析）优化的目标是什么？
最小化重构误差/最大化投影的方差

### 59.LDA（线性判别分析）优化的目标是什么？
最大化类间差异与类内差异

### 60.解释神经网络的万能逼近定理？
只要激活函数选择得到，神经元的数量足够，至少有一个隐含层的神经网络可以逼近闭区间上任意一个连续函数到任意指定的精度。

### 61.softmax回归训练时的目标函数是凸函数吗？
是，但有不止一个全局最优解

### 62.SVM为什么要求解对偶问题？为什么对偶问题与原问题等价？
原问题不容易求解，含有大量的不易处理的不等式约束。原问题满足slater条件，强对偶成立，因此原问题与对偶问题等价。

### 63.神经网络是生成模型还是判别模型？
判别模型，直接输出类别标签，或者输出类后验概率p(y|x)

### 64.Logistic回归是生成模型还是判别模型？
判别模型，直接输出类后验概率p(y|x)，没有对类条件概率p(x|y)或者联合概率p(x, y)建模

### 65.Batch Normalization和Group Normalization有何区别？
BN是在batch这个维度上进行归一化，GN是在计算channel方向每个group的均值和方差

### 66.GAN中模型坍塌（model collapse）是指什么？
模型坍塌，即产生的样本单一，没有了多样性。

### 67.目前GAN训练中存在的主要问题是什么？
（1）训练不易收敛；
（2）模型坍塌；

### 68.Shufflenet为什么效果好？
通过引入“通道重排”增加了组与组之间信息交换。

### 69.模型压缩的主要方法有哪些？
（1）从模型结构上优化：模型剪枝、模型蒸馏、automl直接学习出简单的结构
（2）模型参数量化将FP32的数值精度量化到FP16、INT8、二值网络、三值网络等

### 70.目标检测中IOU是如何计算的？
检测结果与Ground Truth的交集比上它们的并集，即为检测的准确率IOU

### 71.给定0-1矩阵，如何求连通域？
可采用广度优先搜索

### 72.OCR任务中文本序列识别的主流方法是什么？
RNN+CTC

### 73.在神经网络体系结构中，哪些会有权重共享？
（1）卷积神经网络
（2）递归神经网络
（3）全链接网络
（1） && （2）

### 73.一个典型人脸识别系统的识别流程？
人脸检测→人脸对齐→人脸特征提取→人脸特征对比；

### 74.平面内有两个矩形，如何快速计算它们的IOU？
交比并；

### 75.使用深度卷积网络做图像分类如果训练一个拥有1000万个类的模型会碰到什么问题？
特征维度太大，占用的内存/显存比较多；
模型收敛速度；

### 76.HMM(隐马尔科夫模型)和CRF(条件随机场)的区别？
前者描述的是P(X,Y)=P(X|Y)*P(Y)，是generative model；后者描述的P(Y|X)，是discriminative model. 前者你要加入对状态概率分布的先验知识，而后者完全是data driven。

### 77.深度学习中为什么不用二阶导去优化？
Hessian矩阵是n*n，在高维情况下这个矩阵非常大，计算和存储都是问题

### 78.深度学习中的mini-batch的大小对学习效果有何影响？
mini-batch太小会导致收敛变慢，太大容易陷入sharp minimal，泛化性不好。

### 79.线性回归对于数据的假设是怎样的？
（1）线性，y是多个自变量x之间的线性组合；
（2）同方差性，不同的因变量x的方差都是相同的；
（3）弱外生性，假设用来预测的自变量x是没有测量误差的；
（4）预测变量之中没有多重共线性；

### 80.什么是共线性，跟过拟合有啥关联？
**共线性**：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
共线性会造成冗余，导致过拟合。

解决方法：排除变量的相关性/加入权重正则

### 81.Bias和Variance的区别
Bias度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力。

Variance度量了同样大小的训练集的变动所导致的学习性能变化，即刻画了数据扰动所造成的影响。

### 82.对于支持向量机，高斯核一般比线性核有更好的精度，但实际应用中为什么一般用线性核而不用高斯核？
如果训练样本的量很大，训练得到的模型中支持向量的数量太多，在每次做预测时，高斯核需要计算待预测样本与每个支持向量的内积，然后作核函数变换，这会非常耗时；而线性核只需要计算$W^{T}X+b$

### 83.高斯混合模型中，为什么每个高斯分量的权重之和要保证为1？
为了保证这个函数是一个概率密度函数，即积分值为1

### 84.介绍beam search算法的原理
这是一种解码算法，每次选择概率最大的几个解作为候选解，逐步扩展。

### 85.介绍seq2seq的原理
整个系统由两个RNN组成，一个充当编码器，一个充当解码器；编码器依次接收输入的序列数据，当最后一个数据点输入之后，将循环层的状态向量作为语义向量，与解码器网络的输入向量一起，送入解码器中进行预测

### 86.介绍CTC的原理
CTC通过引入空白符号，以及消除连续的相同符号，将RNN原始的输出序列映射为最终的目标序列。可以解决对**未对齐的序列数据**进行预测的问题，如语音识别。

### 87.介绍广义加法模型的原理
广义加法模型用多个基函数的和来拟合目标函数，训练的时候，一次确定每个基函数

### 88.为什么很多时候用正态分布来对随机变量进行建模？
现实世界中很多变量都服从或近似服从正态分布。中心极限定理指出，抽样得到的多个独立同分布的随机变量样本，当样本数趋向于正无穷时，它们的和服从正态分布。


